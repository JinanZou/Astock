{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DIB5Ckvwjno"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5eL_gHOwkAz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:30:55.803983Z",
     "start_time": "2022-02-08T10:30:55.660482Z"
    },
    "id": "Qyy2IpTzbgF0"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:30:59.699352Z",
     "start_time": "2022-02-08T10:30:55.805594Z"
    },
    "id": "dPdpNulr5rdc"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U watermark\n",
    "!pip install -qq transformers\n",
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:01.038190Z",
     "start_time": "2022-02-08T10:30:59.700337Z"
    },
    "id": "pEHKZt-qRG5x"
   },
   "outputs": [],
   "source": [
    "#@title Setup & Config\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from collections import Counter\n",
    "import ast\n",
    "import random\n",
    "import json\n",
    "from math import isnan\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 10\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHAlx_DgsuY6"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoiHKtRV8EwC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:01.040562Z",
     "start_time": "2022-02-08T10:31:01.038911Z"
    },
    "id": "2O_HqSb1Bon0"
   },
   "outputs": [],
   "source": [
    "# sns.countplot(df.label_norm)\n",
    "# plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:05.994993Z",
     "start_time": "2022-02-08T10:31:01.041641Z"
    },
    "id": "xRdWcGGBFYTu"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "PRE_TRAINED_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm-ext\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"ProsusAI/finbert\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case=True)\n",
    "PRE_TRAINED_MODEL_NAME = '/home/bit/stock/model/pretrained-bert/ROBERT_4_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:05.999077Z",
     "start_time": "2022-02-08T10:31:05.995761Z"
    },
    "id": "EW_JmYxCNyLa"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:06.548011Z",
     "start_time": "2022-02-08T10:31:05.999678Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df_train = pd.read_csv('../data/pre/train.csv',sep='\\t')\n",
    "df_val = pd.read_csv('../data/pre/val.csv',sep='\\t')\n",
    "df_test = pd.read_csv('../data/pre/test.csv',sep='\\t')\n",
    "df_ood = pd.read_csv('../data/pre/ood.csv',sep='\\t')\n",
    "\n",
    "\n",
    "df_train = df_train.drop(df_train.loc[df_train.verbA0A1.isna()].index)\n",
    "df_test = df_test.drop(df_test.loc[df_test.verbA0A1.isna()].index)\n",
    "df_val = df_val.drop(df_val.loc[df_val.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "\n",
    "df_train = df_train.drop(df_train.loc[df_train.verbA0A1=='[]'].index)\n",
    "df_test = df_test.drop(df_test.loc[df_test.verbA0A1=='[]'].index)\n",
    "df_val = df_val.drop(df_val.loc[df_val.verbA0A1=='[]'].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.569116Z",
     "start_time": "2022-02-08T10:31:06.548859Z"
    },
    "id": "-zD37Z7TN_ii"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def string_to_tuples_list(text):\n",
    "  if text is np.nan or text =='[]':\n",
    "    return []\n",
    "  text = ''.join(text.split('], ['))\n",
    "  tmp = eval(text.strip('[').strip(']'))\n",
    "  if not isinstance(tmp[0],tuple):\n",
    "    return [tmp]\n",
    "  return list(tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_train[col] = df_train[col].apply(string_to_tuples_list)\n",
    "  df_val[col] = df_val[col].apply(string_to_tuples_list)\n",
    "  df_test[col] = df_test[col].apply(string_to_tuples_list)\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "# for col in ['verbA0A1']:\n",
    "  df_train[col] = df_train[col].apply(ast.literal_eval)\n",
    "  df_val[col] = df_val[col].apply(ast.literal_eval)\n",
    "  df_test[col] = df_test[col].apply(ast.literal_eval)\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "\n",
    "def mask(df):\n",
    "  df = df.reset_index(drop = True)\n",
    "  df['verb_mask'] = 0\n",
    "  df['A0_mask'] = 0\n",
    "  df['A1_mask'] = 0\n",
    "  df['verb_mask'] = df['verb_mask'].astype('object')\n",
    "  df['A0_mask'] = df['A0_mask'].astype('object')\n",
    "  df['A1_mask'] = df['A1_mask'].astype('object')\n",
    "  for index,row in df.iterrows():\n",
    "\n",
    "    df.at[index,'stock_factors'] = [*map(float,df.loc[index,'stock_factors'])]\n",
    "    AV_num = 0\n",
    "    for k,col in enumerate(['verb','A0','A1']):\n",
    "      masks = []\n",
    "      for j in range(len(row['verbA0A1'])):\n",
    "        mask = np.zeros(299)\n",
    "        idx = []\n",
    "        for v in row['verbA0A1'][j][k]:\n",
    "          \n",
    "          idx = idx + [int(i) for i in range(v[0],v[0]+v[1])]\n",
    "        # idx = np.unique(idx).tolist()\n",
    "        counter = Counter(idx)\n",
    "\n",
    "        mask = [0 if counter[i]== 0 else 1/len(counter) for i in range(0,len(mask))]\n",
    "        mask.insert(0,0)\n",
    "        masks.append(mask)\n",
    "      AV_num = len(masks)\n",
    "      for i in range(10 - len(masks)):\n",
    "        masks.append(np.zeros(300))\n",
    "      while len(masks)>10:\n",
    "        masks.pop()\n",
    "      name = col+'_mask'\n",
    "      df.at[index,name] = np.array(masks)\n",
    "    if AV_num>10:\n",
    "      AV_num=10\n",
    "    df.loc[index,'AV_num'] = int(AV_num)\n",
    "  df.AV_num = df.AV_num.astype('int')\n",
    "#   df.stock_factors = df.stock_factors.apply(np.array)\n",
    "  return df\n",
    "\n",
    "\n",
    "df_train = mask(df_train)\n",
    "df_test = mask(df_test)\n",
    "df_val = mask(df_val)\n",
    "df_ood = mask(df_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.574892Z",
     "start_time": "2022-02-08T10:31:32.570081Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "class_names = ['negative','neutral', 'positive']\n",
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets,verb,A0,A1,AV_num,tokenizer,stock_factors, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.stock_factors = stock_factors\n",
    "    self.verb = verb\n",
    "    self.A0 = A0\n",
    "    self.A1 = A1\n",
    "    self.AV_num = AV_num\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "    stock_factors = self.stock_factors[item]\n",
    "    v = self.verb[item]\n",
    "    a0 = self.A0[item]\n",
    "    a1 = self.A1[item]\n",
    "    av_num = self.AV_num[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long),\n",
    "      'stock_factors':torch.tensor(stock_factors),\n",
    "      'verb': torch.tensor(v),\n",
    "      'A0': torch.tensor(a0),\n",
    "      'A1': torch.tensor(a1),\n",
    "      'AV_num': torch.tensor(av_num)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.588806Z",
     "start_time": "2022-02-08T10:31:32.575686Z"
    },
    "id": "cPZSIigpPtEr"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=df.text_a.to_numpy(),\n",
    "    targets=df.label.to_numpy(),\n",
    "    stock_factors = df.stock_factors,\n",
    "    verb = df.verb_mask,\n",
    "    A0 = df.A0_mask,\n",
    "    A1 = df.A1_mask,\n",
    "    AV_num = df.AV_num,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    # num_workers=4,\n",
    "    shuffle=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.594270Z",
     "start_time": "2022-02-08T10:31:32.589959Z"
    },
    "id": "5D6CwQDIAk0C"
   },
   "outputs": [],
   "source": [
    "# df_train.sample(n=4000).to_csv('/content/drive/MyDrive/data/df_train_srl.csv',sep='\\t',index=None)\n",
    "# df_test.sample(n=500).to_csv('/content/drive/MyDrive/data/df_test_srl.csv',sep='\\t',index=None)\n",
    "# df_val.sample(n=500).to_csv('/content/drive/MyDrive/data/df_val_srl.csv',sep='\\t',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.610052Z",
     "start_time": "2022-02-08T10:31:32.595401Z"
    },
    "id": "tm2qZ9OofxC9"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, max_len, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, max_len, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, max_len, BATCH_SIZE)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bidtjT6UEKkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.614790Z",
     "start_time": "2022-02-08T10:31:32.610966Z"
    },
    "id": "hsRtLnPlWulS"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,output_attentions=True)\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "    self.sig = nn.Sigmoid()\n",
    "    # self.relu = nn.ReLU()\n",
    "    # self.L1 = nn.Linear(self.bert.config.hidden_size,self.bert.config.hidden_size//2)\n",
    "    self.out1 = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )[1]\n",
    "\n",
    "#     print(result.keys())\n",
    "\n",
    "#     attention_weight = torch.sum(result.attentions[11][:,:,:,:],1)[:,0,:]\n",
    "#     print(pooled_output.keys())\n",
    "\n",
    "#     output = torch.mean(pooled_output,1)\n",
    "#     output = self.drop(pooled_output)\n",
    "    # # output = self.L1(output)\n",
    "#     output = self.out(output)\n",
    "#     output = self.flatten(transformer_output.float())\n",
    "    output = self.sig(pooled_output)\n",
    "#     output = self.drop(output)\n",
    "    output = self.out1(output)\n",
    "    output = self.sig(output)\n",
    "    output = self.drop(output)\n",
    "    output = self.out(output)\n",
    " \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.548891Z",
     "start_time": "2022-02-08T10:31:32.616542Z"
    },
    "id": "KQEOV2pfYBPs"
   },
   "outputs": [],
   "source": [
    "model = SentimentClassifier(3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.570851Z",
     "start_time": "2022-02-08T10:31:35.549862Z"
    },
    "id": "IfJO5a0NGPf_"
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.574414Z",
     "start_time": "2022-02-08T10:31:35.571853Z"
    },
    "id": "uLl-umJKYGtB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.016876Z",
     "start_time": "2022-02-08T10:31:35.575101Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model(input_ids, attention_mask)\n",
    "\n",
    "# output.attentions[0][0].shape\n",
    "\n",
    "# len(output.attentions)\n",
    "\n",
    "# output.attentions[0][:,:,:,:]\n",
    "\n",
    "# torch.sum(output.attentions[0][:,:,:,:],1)[:,1,:]\n",
    "\n",
    "# output.attentions[0]\n",
    "\n",
    "# F.softmax(output[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82irQeXrOfWI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.024088Z",
     "start_time": "2022-02-08T10:31:36.017752Z"
    },
    "id": "4q2fUr9TYMxW"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYA1f6t3f2ex"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.030991Z",
     "start_time": "2022-02-08T10:31:36.025271Z"
    },
    "id": "oLLFWDvDYeeS"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.036544Z",
     "start_time": "2022-02-08T10:31:36.032036Z"
    },
    "id": "VmDYxiXDYl-u"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.041834Z",
     "start_time": "2022-02-08T10:31:36.037208Z"
    },
    "id": "Nkvo-2p93Dnr"
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "# EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.357202Z",
     "start_time": "2022-02-08T10:31:36.042799Z"
    },
    "id": "VZvKK2TkYojZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'Pretrained_RoBERT.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.360820Z",
     "start_time": "2022-02-08T10:53:38.358158Z"
    },
    "id": "FIyLcNepYr4f"
   },
   "outputs": [],
   "source": [
    "len(history[\"train_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.507047Z",
     "start_time": "2022-02-08T10:53:38.361525Z"
    },
    "id": "xu7Y2BPzLZgi"
   },
   "outputs": [],
   "source": [
    "plt.plot([i.cpu() for i in history['train_acc']], label='train accuracy')\n",
    "plt.plot([i.cpu() for i in history['val_acc']], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq1lJWndW3YH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.509825Z",
     "start_time": "2022-02-08T10:53:38.508040Z"
    },
    "id": "mwwZzIqBfNvG"
   },
   "outputs": [],
   "source": [
    "class_names={'negative','neutral','positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:39.268017Z",
     "start_time": "2022-02-08T10:53:38.510561Z"
    },
    "id": "YYMhWiIrBrV4"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1V8itWtowCYnb2Bc9KlK9SxGff9WwmogA\n",
    "\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('Pretrained_RoBERT.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:39.271069Z",
     "start_time": "2022-02-08T10:53:39.268960Z"
    },
    "id": "KYRPY0C-fFQh"
   },
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:44.712567Z",
     "start_time": "2022-02-08T10:53:39.271757Z"
    },
    "id": "c7Cckbz8GbDK"
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:44.734064Z",
     "start_time": "2022-02-08T10:53:44.713733Z"
    }
   },
   "outputs": [],
   "source": [
    "# text=df_test.loc[df_test.text_a.apply(len)<100].sample(n=1).text_a.values[0]\n",
    "\n",
    "# text_final = ''\n",
    "# for i in tokenizer.tokenize(text):\n",
    "#     tem = ' '+i\n",
    "#     text_final+=tem\n",
    "\n",
    "# text_final = text_final[1:]\n",
    "\n",
    "\n",
    "\n",
    "# #Credits to Lin Zhouhan(@hantek) for the complete visualization code\n",
    "# import random, os, numpy, scipy\n",
    "# from codecs import open\n",
    "# def createHTML(texts, weights, fileName):\n",
    "#     \"\"\"\n",
    "#     Creates a html file with text heat.\n",
    "# weights: attention weights for visualizing\n",
    "# texts: text on which attention weights are to be visualized\n",
    "#     \"\"\"\n",
    "#     fileName = \"./visualization/\"+fileName\n",
    "#     fOut = open(fileName, \"w\", encoding=\"utf-8\")\n",
    "#     part1 = \"\"\"\n",
    "#     <html lang=\"zh-Hans\">\n",
    "#     <head>\n",
    "#     <meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\">\n",
    "#     <style>\n",
    "#     body {\n",
    "#     font-family: Sans-Serif;\n",
    "#     }\n",
    "#     </style>\n",
    "#     </head>\n",
    "#     <body>\n",
    "#     <h3>\n",
    "#     Heatmaps\n",
    "#     </h3>\n",
    "#     </body>\n",
    "#     <script>\n",
    "#     \"\"\"\n",
    "#     part2 = \"\"\"\n",
    "#     var color = \"255,0,0\";\n",
    "#     var ngram_length = 3;\n",
    "#     var half_ngram = 1;\n",
    "#     for (var k=0; k < any_text.length; k++) {\n",
    "#     var tokens = any_text[k].split(\" \");\n",
    "#     var intensity = new Array(tokens.length);\n",
    "#     var max_intensity = Number.MIN_SAFE_INTEGER;\n",
    "#     var min_intensity = Number.MAX_SAFE_INTEGER;\n",
    "#     for (var i = 0; i < intensity.length; i++) {\n",
    "#     intensity[i] = 0.0;\n",
    "#     for (var j = -half_ngram; j < ngram_length-half_ngram; j++) {\n",
    "#     if (i+j < intensity.length && i+j > -1) {\n",
    "#     intensity[i] += trigram_weights[k][i + j];\n",
    "#     }\n",
    "#     }\n",
    "#     if (i == 0 || i == intensity.length-1) {\n",
    "#     intensity[i] /= 2.0;\n",
    "#     } else {\n",
    "#     intensity[i] /= 3.0;\n",
    "#     }\n",
    "#     if (intensity[i] > max_intensity) {\n",
    "#     max_intensity = intensity[i];\n",
    "#     }\n",
    "#     if (intensity[i] < min_intensity) {\n",
    "#     min_intensity = intensity[i];\n",
    "#     }\n",
    "#     }\n",
    "#     var denominator = max_intensity - min_intensity;\n",
    "#     for (var i = 0; i < intensity.length; i++) {\n",
    "#     intensity[i] = (intensity[i] - min_intensity) / denominator;\n",
    "#     }\n",
    "#     if (k%2 == 0) {\n",
    "#     var heat_text = \"<p><br><b>Example:</b><br>\";\n",
    "#     } else {\n",
    "#     var heat_text = \"<b>Example:</b><br>\";\n",
    "#     }\n",
    "#     var space = \" \";\n",
    "#     for (var i = 0; i < tokens.length; i++) {\n",
    "#     heat_text += \"<span style='background-color:rgba(\" + color + \",\" + intensity[i] + \")'>\"  + tokens[i] + \"</span>\";\n",
    "#     if (space == \"\") {\n",
    "#     space = \" \";\n",
    "#     }\n",
    "#     }\n",
    "#     //heat_text += \"<p>\";\n",
    "#     document.body.innerHTML += heat_text;\n",
    "#     }\n",
    "#     </script>\n",
    "#     </html>\"\"\"\n",
    "#     putQuote = lambda x: \"\\\"%s\\\"\"%x\n",
    "#     textsString = \"var any_text = [%s];\\n\"%(\",\".join(map(putQuote, texts)))\n",
    "#     weightsString = \"var trigram_weights = [%s];\\n\"%(\",\".join(map(str,weights)))\n",
    "# #     print(part1)\n",
    "# #     print(textsString)\n",
    "# #     print(part2)\n",
    "# #     print(weightsString)\n",
    "#     fOut.write(part1)\n",
    "#     fOut.write(textsString)\n",
    "#     fOut.write(weightsString)\n",
    "#     fOut.write(part2)\n",
    "#     fOut.close()\n",
    "  \n",
    "#     return\n",
    "# # ===================================================\n",
    "# # 2. add text and get weight\n",
    "# # ===================================================\n",
    "# text1 = text_final\n",
    "# tok = tokenizer.tokenize(text1)\n",
    "# # text2 = 'NEWS'\n",
    "# # p_pos = len(tok1)\n",
    "# # tok2 = tokenizer.tokenize(text2)\n",
    "# # tok = tok1+tok2\n",
    "\n",
    "\n",
    "# ids = torch.tensor(tokenizer.convert_tokens_to_ids(tok)).unsqueeze(0).to('cuda')\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model.bert(ids)\n",
    "# attentions = torch.cat(output[2]).to('cpu')\n",
    "\n",
    "# # =================================================\n",
    "# # 3. creat html attention heat map\n",
    "# # =================================================\n",
    "# createHTML([text1],[attentions[11,3,0,1:-2].numpy().tolist()],'Pretrained_RoBERT.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwYAGJUiiuxv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:55.246232Z",
     "start_time": "2022-02-08T08:44:55.241813Z"
    },
    "id": "aKwdpn1PGfIy"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neshR7Q9DXGK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:58.094492Z",
     "start_time": "2022-02-08T08:44:58.092104Z"
    },
    "id": "UIwgaRKNDPcS"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:45:00.972382Z",
     "start_time": "2022-02-08T08:45:00.941048Z"
    },
    "id": "YkPHXF0MGiCh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names,digits = 4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.552144Z",
     "start_time": "2022-02-08T08:44:27.552138Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.552650Z",
     "start_time": "2022-02-08T08:44:27.552644Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ood = pd.read_csv('../data/pre/ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-01-01')&(df_ood.DATE<='2021-03-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('../data/pre/ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-04-01')&(df_ood.DATE<='2021-06-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('../data/pre/ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-07-01')&(df_ood.DATE<='2021-09-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('../data/pre/ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[df_ood.DATE>='2021-10-01']\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.553284Z",
     "start_time": "2022-02-08T08:44:27.553278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ood = pd.read_csv('../data/df_all_year_srl.csv',sep='\\t')\n",
    "# df_ood = df_ood.loc[(df_ood.DATE>='2021-05-05')&(df_ood.DATE<='2021-09-01')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=class_names, columns=class_names)\n",
    "show_confusion_matrix(df_ood_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.553858Z",
     "start_time": "2022-02-08T08:44:27.553852Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([y_ood_review_texts, y_ood_pred.numpy(), y_ood_pred_probs.numpy(), y_ood.numpy()]).T\n",
    "df = df.rename(columns={0:'text',1:'prediction',2:'probability',3:'labels'})\n",
    "df.to_csv('Pretrained_RoBERT_ood.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T09:39:06.892477Z",
     "start_time": "2022-02-08T09:39:06.887447Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:50:07.483464Z",
     "start_time": "2022-02-08T08:50:07.480347Z"
    }
   },
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T09:36:09.123555Z",
     "start_time": "2022-02-08T09:36:09.118807Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.555237Z",
     "start_time": "2022-02-08T08:44:27.555230Z"
    },
    "id": "6QZ0EgQoalz2"
   },
   "outputs": [],
   "source": [
    "# !pip install lit-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.555793Z",
     "start_time": "2022-02-08T08:44:27.555787Z"
    },
    "id": "eAeHdVF7c-QF"
   },
   "outputs": [],
   "source": [
    "# from lit_nlp.api.dataset import Dataset\n",
    "# class MultiNLIData(Dataset):\n",
    "#   \"\"\"Loader for MultiNLI development set.\"\"\"\n",
    "#   def __init__(self, df):\n",
    "#     # Read the eval set from a .tsv file as distributed with the GLUE benchmark.\n",
    "#     # df = pandas.read_csv(path, sep='\\t')\n",
    "#     # Store as a list of dicts, conforming to self.spec()\n",
    "#     self.LABELS = [0,1]\n",
    "#     self._examples = [{\n",
    "#       'sentence': row['DESCRIPTION_EN'],\n",
    "#       # 'hypothesis': row['sentence2'],\n",
    "#       'label': row['LABEL']\n",
    "#       # 'genre': row['genre'],\n",
    "#     } for _, row in df.iterrows()]\n",
    "#   def spec(self):\n",
    "#     return {\n",
    "#       'sentence': lit_types.TextSegment(),\n",
    "#       # 'hypothesis': lit_types.TextSegment(),\n",
    "#       'label': lit_types.CategoryLabel(vocab=self.LABELS),\n",
    "#       # We can include additional fields, which don't have to be used by the model.\n",
    "#       # 'genre': lit_types.Label(),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5886G4mdXia"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrSBCdqO0sUs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-nX4zrOdxpo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARdnz5td18VV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iMviJZsfdifG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h5I2QKodj8b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-abzg6avOQq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flJ1xtWNvYej"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.556302Z",
     "start_time": "2022-02-08T08:44:27.556296Z"
    },
    "id": "nBxb2zhWvfaK"
   },
   "outputs": [],
   "source": [
    "# from absl import app\n",
    "# from absl import flags\n",
    "# from absl import logging\n",
    "\n",
    "# from lit_nlp import dev_server\n",
    "# from lit_nlp import server_flags\n",
    "# from lit_nlp.api import model as lit_model\n",
    "# from lit_nlp.api import types as lit_types\n",
    "# # Use the regular GLUE data loaders, because these are very simple already.\n",
    "# from lit_nlp.examples.datasets import glue\n",
    "# from lit_nlp.lib import utils\n",
    "\n",
    "# import torch\n",
    "# import transformers\n",
    "\n",
    "# # NOTE: additional flags defined in server_flags.py\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"model_path\",\n",
    "#     \"https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz\",\n",
    "#     \"Path to trained model, in standard transformers format, e.g. as \"\n",
    "#     \"saved by model.save_pretrained() and tokenizer.save_pretrained()\")\n",
    "\n",
    "\n",
    "# def _from_pretrained(cls, *args, **kw):\n",
    "#   \"\"\"Load a transformers model in PyTorch, with fallback to TF2/Keras weights.\"\"\"\n",
    "#   try:\n",
    "#     return cls.from_pretrained(*args, **kw)\n",
    "#   except OSError as e:\n",
    "#     logging.warning(\"Caught OSError loading model: %s\", e)\n",
    "#     logging.warning(\n",
    "#         \"Re-trying to convert from TensorFlow checkpoint (from_tf=True)\")\n",
    "#     return cls.from_pretrained(*args, from_tf=True, **kw)\n",
    "\n",
    "\n",
    "# class SimpleSentimentModel(lit_model.Model):\n",
    "#   \"\"\"Simple sentiment analysis model.\"\"\"\n",
    "\n",
    "#   LABELS = [0,1]  # negative, positive\n",
    "\n",
    "#   def __init__(self, model_name_or_path):\n",
    "#     self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#         model_name_or_path)\n",
    "#     model_config = transformers.AutoConfig.from_pretrained(\n",
    "#         model_name_or_path,\n",
    "#         num_labels=2,\n",
    "#         output_hidden_states=True,\n",
    "#         output_attentions=True,\n",
    "#     )\n",
    "#     # This is a just a regular PyTorch model.\n",
    "#     self.model = _from_pretrained(\n",
    "#         transformers.AutoModelForSequenceClassification,\n",
    "#         model_name_or_path,\n",
    "#         config=model_config)\n",
    "#     self.model.eval()\n",
    "\n",
    "#   ##\n",
    "#   # LIT API implementation\n",
    "#   def max_minibatch_size(self):\n",
    "#     # This tells lit_model.Model.predict() how to batch inputs to\n",
    "#     # predict_minibatch().\n",
    "#     # Alternately, you can just override predict() and handle batching yourself.\n",
    "#     return 16\n",
    "\n",
    "#   def predict_minibatch(self, inputs):\n",
    "#     # Preprocess to ids and masks, and make the input batch.\n",
    "#     encoded_input = self.tokenizer.batch_encode_plus(\n",
    "#         [ex[\"sentence\"] for ex in inputs],\n",
    "#         return_tensors=\"pt\",\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=200,\n",
    "#         padding=\"longest\",\n",
    "#         truncation=\"longest_first\")\n",
    "\n",
    "#     # Check and send to cuda (GPU) if available\n",
    "#     if torch.cuda.is_available():\n",
    "#       self.model.cuda()\n",
    "#       for tensor in encoded_input:\n",
    "#         encoded_input[tensor] = encoded_input[tensor].cuda()\n",
    "#     # Run a forward pass.\n",
    "#     with torch.no_grad():  # remove this if you need gradients.\n",
    "#       out: transformers.modeling_outputs.SequenceClassifierOutput = \\\n",
    "#           self.model(**encoded_input)\n",
    "\n",
    "#     # Post-process outputs.\n",
    "#     batched_outputs = {\n",
    "#         \"probas\": torch.nn.functional.softmax(out.logits, dim=-1),\n",
    "#         \"input_ids\": encoded_input[\"input_ids\"],\n",
    "#         \"ntok\": torch.sum(encoded_input[\"attention_mask\"], dim=1),\n",
    "#         \"cls_emb\": out.hidden_states[-1][:, 0],  # last layer, first token\n",
    "#     }\n",
    "#     # Return as NumPy for further processing.\n",
    "#     detached_outputs = {k: v.cpu().numpy() for k, v in batched_outputs.items()}\n",
    "#     # Unbatch outputs so we get one record per input example.\n",
    "#     for output in utils.unbatch_preds(detached_outputs):\n",
    "#       ntok = output.pop(\"ntok\")\n",
    "#       output[\"tokens\"] = self.tokenizer.convert_ids_to_tokens(\n",
    "#           output.pop(\"input_ids\")[1:ntok - 1])\n",
    "#       yield output\n",
    "\n",
    "#   def input_spec(self) -> lit_types.Spec:\n",
    "#     return {\n",
    "#         \"sentence\": lit_types.TextSegment(),\n",
    "#         \"label\": lit_types.CategoryLabel(vocab=self.LABELS, required=False)\n",
    "#     }\n",
    "\n",
    "#   def output_spec(self) -> lit_types.Spec:\n",
    "#     return {\n",
    "#         \"tokens\": lit_types.Tokens(),\n",
    "#         \"probas\": lit_types.MulticlassPreds(parent=\"label\", vocab=self.LABELS),\n",
    "#         \"cls_emb\": lit_types.Embeddings()\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.556809Z",
     "start_time": "2022-02-08T08:44:27.556803Z"
    },
    "id": "B64IFx5g461l"
   },
   "outputs": [],
   "source": [
    "# dataset = MultiNLIData(df_train.sample(1800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.557289Z",
     "start_time": "2022-02-08T08:44:27.557283Z"
    },
    "id": "qR0nbr94v2xz"
   },
   "outputs": [],
   "source": [
    "# datasets = {\"sst_dev\": dataset}\n",
    "# models = {\"sst\": SimpleSentimentModel(\"/content/drive/MyDrive/model/best_model/\")}\n",
    "# # models = {\"sst\": SimpleSentimentModel(\"bert-base-uncased\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.557937Z",
     "start_time": "2022-02-08T08:44:27.557931Z"
    },
    "id": "WR8VwsH9v4l8"
   },
   "outputs": [],
   "source": [
    "# from lit_nlp import notebook\n",
    "# widget = notebook.LitWidget(models, datasets, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.558409Z",
     "start_time": "2022-02-08T08:44:27.558403Z"
    },
    "id": "wNPctrXDBQSM"
   },
   "outputs": [],
   "source": [
    "# mm = models[\"sst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.558958Z",
     "start_time": "2022-02-08T08:44:27.558952Z"
    },
    "id": "lHX6hJtVCAuX"
   },
   "outputs": [],
   "source": [
    "# mm.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.559385Z",
     "start_time": "2022-02-08T08:44:27.559379Z"
    },
    "id": "cihtCCxC3ftN"
   },
   "outputs": [],
   "source": [
    "# widget.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.559986Z",
     "start_time": "2022-02-08T08:44:27.559980Z"
    },
    "id": "h5c4S99U5UOS"
   },
   "outputs": [],
   "source": [
    "# dataset.LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.560500Z",
     "start_time": "2022-02-08T08:44:27.560494Z"
    },
    "id": "WxSfPfOF7dNo"
   },
   "outputs": [],
   "source": [
    "# df.loc[df.NAME==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHZ2GHS2YPJx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pretrained_RoBERT.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "17ul_8r9Y_9ICAGtq-Wv-F4BtTFqck-ul",
     "timestamp": 1634181641588
    },
    {
     "file_id": "1bn3tYjWp7f5Uc_K5Qwkm9Z-Hrp9TeO0T",
     "timestamp": 1633307974918
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
